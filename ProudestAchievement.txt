My proudest achievement was while trying to solve a bug that caused duplication of records in one of Symantec's ingestion data pipeline. The problem was that the number of records in our database has recently spiked and we were asked to check for any issues. The most obvious techniques that everyone started looking at were to check for bugs in the recently changed code and query database to find what might the error be. While this was happening, I started to look for ways to find an issue in a Storm topology and came across 'Storm metrics', which was usually for Storm's internal use but was configurable. So I made a few code changes and put counters after each processing bolt that would count the number of records processed by that bolt. Using an already existing log monitoring system, I added these metrics and have them visualized on a graph. From there I could see the count of records after each process and found a spike after a particular process. Then I just had to look at a small piece of code to find the bug that was duplicating the records. 
The reason I am proud of this is that I realized that the conventional techniques mentioned won't work in such a large scale system where millions of records are added each day and multiple programmers submit code that gets merged in the same master branch. This solution was later integrated as a part of the complete system and has helped in resolving various other bugs.